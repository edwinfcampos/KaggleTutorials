{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Handling Missing Values\n",
    "\n",
    "from https://www.kaggle.com/dansbecker/handling-missing-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, detect which cells have missing values, and then count how many there are in each column with the command:\n",
    "\n",
    "missing_val_count_by_column = (data.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0]\n",
    "\n",
    "\n",
    "#### Second, refill missing values...\n",
    "\n",
    "#### 1) A Simple Option: Drop Columns with Missing Values\n",
    "\n",
    "    data_without_missing_values = original_data.dropna(axis=1)\n",
    "\n",
    "or\n",
    "\n",
    "    cols_with_missing = [col for col in original_data.columns\n",
    "\n",
    "                                 if original_data[col].isnull().any()]\n",
    "\n",
    "    redued_original_data = original_data.drop(cols_with_missing, axis=1)\n",
    "\n",
    "    reduced_test_data = test_data.drop(cols_with_missing, axis=1)\n",
    "\n",
    "#### 2) A Better Option: Imputation\n",
    "\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    my_imputer = SimpleImputer()\n",
    "\n",
    "    data_with_imputed_values = my_imputer.fit_transform(original_data)\n",
    "\n",
    "#### 3) An Extension To Imputation\n",
    "\n",
    "###### make copy to avoid changing original data (when Imputing)\n",
    "    new_data = original_data.copy()\n",
    "\n",
    "###### make new columns indicating what will be imputed\n",
    "    cols_with_missing = (col for col in new_data.columns\n",
    "\n",
    "                                 if new_data[col].isnull().any())\n",
    "\n",
    "    for col in cols_with_missing:\n",
    "\n",
    "    new_data[col + '_was_missing'] = new_data[col].isnull()\n",
    "\n",
    "##### Imputation\n",
    "    my_imputer = SimpleImputer()\n",
    "\n",
    "    new_data = pd.DataFrame(my_imputer.fit_transform(new_data))\n",
    "\n",
    "    new_data.columns = original_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "melb_data = pd.read_csv('./melb_data.csv')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "melb_target = melb_data.Price\n",
    "melb_predictors = melb_data.drop(['Price'], axis=1)\n",
    "\n",
    "# For the sake of keeping the example simple, we'll use only numeric predictors. \n",
    "melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Function to Measure Quality of An Approach\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(melb_numeric_predictors, \n",
    "                                                    melb_target,\n",
    "                                                    train_size=0.7, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "\n",
    "def score_dataset(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files will be used in the second part\n",
    "type(X_train) will give... pandas.core.frame.DataFrame \n",
    "    out_train = X_train\n",
    "    out_train.loc[:,'SalePrice'] = y_train # Add new column\n",
    "    out_test = X_test\n",
    "    out_test.loc[:,'SalePrice'] = y_test # Add new column\n",
    "    out_train.to_csv('./train.csv', sep=',')\n",
    "    out_test.to_csv('./test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from dropping columns with Missing Values:\n",
      "188532.9994751853\n"
     ]
    }
   ],
   "source": [
    "# Get Model Score from Dropping Columns with Missing Values\n",
    "cols_with_missing = [col for col in X_train.columns \n",
    "                                 if X_train[col].isnull().any()]\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_test  = X_test.drop(cols_with_missing, axis=1)\n",
    "print(\"Mean Absolute Error from dropping columns with Missing Values:\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation:\n",
      "182282.04273441332\n"
     ]
    }
   ],
   "source": [
    "# Get Model Score from Imputation\n",
    "import sklearn\n",
    "if sklearn.__version__ <= '0.19.2':\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    my_imputer = Imputer()\n",
    "else:  # Valid for sklearn.__version__ >= '0.21.dev0'\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    my_imputer = SimpleImputer()\n",
    "\n",
    "imputed_X_train = my_imputer.fit_transform(X_train)\n",
    "imputed_X_test = my_imputer.transform(X_test)\n",
    "print(\"Mean Absolute Error from Imputation:\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from Imputation while Track What Was Imputed:\n",
      "184628.39606096732\n"
     ]
    }
   ],
   "source": [
    "# Get Score from Imputation with Extra Columns Showing What Was Imputed\n",
    "imputed_X_train_plus = X_train.copy()\n",
    "imputed_X_test_plus = X_test.copy()\n",
    "\n",
    "cols_with_missing = (col for col in X_train.columns \n",
    "                                 if X_train[col].isnull().any())\n",
    "for col in cols_with_missing:\n",
    "    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n",
    "    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "#my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\n",
    "imputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n",
    "\n",
    "print(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Categorical Data with One Hot Encoding\n",
    "\n",
    "from https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "\n",
    "# Drop houses where the target is missing\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "target = train_data.SalePrice\n",
    "\n",
    "# Since missing values isn't the focus of this tutorial, we use the simplest\n",
    "# possible approach, which drops these columns. \n",
    "# For more detail (and a better approach) to missing values, see\n",
    "# https://www.kaggle.com/dansbecker/handling-missing-values\n",
    "cols_with_missing = [col for col in train_data.columns \n",
    "                                 if train_data[col].isnull().any()]                                  \n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n",
    "\n",
    "# \"cardinality\" means the number of unique values in a column.\n",
    "# We use it as our only way to select categorical columns here. This is convenient, though\n",
    "# a little arbitrary.\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "test_predictors = candidate_test_predictors[my_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LandSlope        object\n",
       "Foundation       object\n",
       "BldgType         object\n",
       "OpenPorchSF       int64\n",
       "SaleCondition    object\n",
       "SaleType         object\n",
       "FullBath          int64\n",
       "ExterCond        object\n",
       "HouseStyle       object\n",
       "3SsnPorch         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictors.dtypes.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460 1460\n",
      "      MSSubClass  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
      "0             60     8450            7            5       2003          2003   \n",
      "1             20     9600            6            8       1976          1976   \n",
      "2             60    11250            7            5       2001          2002   \n",
      "3             70     9550            7            5       1915          1970   \n",
      "4             60    14260            8            5       2000          2000   \n",
      "5             50    14115            5            5       1993          1995   \n",
      "6             20    10084            8            5       2004          2005   \n",
      "7             60    10382            7            6       1973          1973   \n",
      "8             50     6120            7            5       1931          1950   \n",
      "9            190     7420            5            6       1939          1950   \n",
      "10            20    11200            5            5       1965          1965   \n",
      "11            60    11924            9            5       2005          2006   \n",
      "12            20    12968            5            6       1962          1962   \n",
      "13            20    10652            7            5       2006          2007   \n",
      "14            20    10920            6            5       1960          1960   \n",
      "15            45     6120            7            8       1929          2001   \n",
      "16            20    11241            6            7       1970          1970   \n",
      "17            90    10791            4            5       1967          1967   \n",
      "18            20    13695            5            5       2004          2004   \n",
      "19            20     7560            5            6       1958          1965   \n",
      "20            60    14215            8            5       2005          2006   \n",
      "21            45     7449            7            7       1930          1950   \n",
      "22            20     9742            8            5       2002          2002   \n",
      "23           120     4224            5            7       1976          1976   \n",
      "24            20     8246            5            8       1968          2001   \n",
      "25            20    14230            8            5       2007          2007   \n",
      "26            20     7200            5            7       1951          2000   \n",
      "27            20    11478            8            5       2007          2008   \n",
      "28            20    16321            5            6       1957          1997   \n",
      "29            30     6324            4            6       1927          1950   \n",
      "...          ...      ...          ...          ...        ...           ...   \n",
      "1430          60    21930            5            5       2005          2005   \n",
      "1431         120     4928            6            6       1976          1976   \n",
      "1432          30    10800            4            6       1927          2007   \n",
      "1433          60    10261            6            5       2000          2000   \n",
      "1434          20    17400            5            5       1977          1977   \n",
      "1435          20     8400            6            9       1962          2005   \n",
      "1436          20     9000            4            6       1971          1971   \n",
      "1437          20    12444            8            5       2008          2008   \n",
      "1438          20     7407            6            7       1957          1996   \n",
      "1439          60    11584            7            6       1979          1979   \n",
      "1440          70    11526            6            7       1922          1994   \n",
      "1441         120     4426            6            5       2004          2004   \n",
      "1442          60    11003           10            5       2008          2008   \n",
      "1443          30     8854            6            6       1916          1950   \n",
      "1444          20     8500            7            5       2004          2004   \n",
      "1445          85     8400            6            5       1966          1966   \n",
      "1446          20    26142            5            7       1962          1962   \n",
      "1447          60    10000            8            5       1995          1996   \n",
      "1448          50    11767            4            7       1910          2000   \n",
      "1449         180     1533            5            7       1970          1970   \n",
      "1450          90     9000            5            5       1974          1974   \n",
      "1451          20     9262            8            5       2008          2009   \n",
      "1452         180     3675            5            5       2005          2005   \n",
      "1453          20    17217            5            5       2006          2006   \n",
      "1454          20     7500            7            5       2004          2005   \n",
      "1455          60     7917            6            5       1999          2000   \n",
      "1456          20    13175            6            6       1978          1988   \n",
      "1457          70     9042            7            9       1941          2006   \n",
      "1458          20     9717            5            6       1950          1996   \n",
      "1459          20     9937            5            6       1965          1965   \n",
      "\n",
      "      BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF          ...            \\\n",
      "0            706           0        150          856          ...             \n",
      "1            978           0        284         1262          ...             \n",
      "2            486           0        434          920          ...             \n",
      "3            216           0        540          756          ...             \n",
      "4            655           0        490         1145          ...             \n",
      "5            732           0         64          796          ...             \n",
      "6           1369           0        317         1686          ...             \n",
      "7            859          32        216         1107          ...             \n",
      "8              0           0        952          952          ...             \n",
      "9            851           0        140          991          ...             \n",
      "10           906           0        134         1040          ...             \n",
      "11           998           0        177         1175          ...             \n",
      "12           737           0        175          912          ...             \n",
      "13             0           0       1494         1494          ...             \n",
      "14           733           0        520         1253          ...             \n",
      "15             0           0        832          832          ...             \n",
      "16           578           0        426         1004          ...             \n",
      "17             0           0          0            0          ...             \n",
      "18           646           0        468         1114          ...             \n",
      "19           504           0        525         1029          ...             \n",
      "20             0           0       1158         1158          ...             \n",
      "21             0           0        637          637          ...             \n",
      "22             0           0       1777         1777          ...             \n",
      "23           840           0        200         1040          ...             \n",
      "24           188         668        204         1060          ...             \n",
      "25             0           0       1566         1566          ...             \n",
      "26           234         486        180          900          ...             \n",
      "27          1218           0        486         1704          ...             \n",
      "28          1277           0        207         1484          ...             \n",
      "29             0           0        520          520          ...             \n",
      "...          ...         ...        ...          ...          ...             \n",
      "1430           0           0        732          732          ...             \n",
      "1431         958           0          0          958          ...             \n",
      "1432           0           0        656          656          ...             \n",
      "1433           0           0        936          936          ...             \n",
      "1434         936           0        190         1126          ...             \n",
      "1435           0           0       1319         1319          ...             \n",
      "1436         616           0        248          864          ...             \n",
      "1437        1336           0        596         1932          ...             \n",
      "1438         600           0        312          912          ...             \n",
      "1439         315         110        114          539          ...             \n",
      "1440           0           0        588          588          ...             \n",
      "1441         697           0        151          848          ...             \n",
      "1442         765           0        252         1017          ...             \n",
      "1443           0           0        952          952          ...             \n",
      "1444           0           0       1422         1422          ...             \n",
      "1445         187         627          0          814          ...             \n",
      "1446         593           0        595         1188          ...             \n",
      "1447        1079           0        141         1220          ...             \n",
      "1448           0           0        560          560          ...             \n",
      "1449         553           0         77          630          ...             \n",
      "1450           0           0        896          896          ...             \n",
      "1451           0           0       1573         1573          ...             \n",
      "1452         547           0          0          547          ...             \n",
      "1453           0           0       1140         1140          ...             \n",
      "1454         410           0        811         1221          ...             \n",
      "1455           0           0        953          953          ...             \n",
      "1456         790         163        589         1542          ...             \n",
      "1457         275           0        877         1152          ...             \n",
      "1458          49        1029          0         1078          ...             \n",
      "1459         830         290        136         1256          ...             \n",
      "\n",
      "      SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
      "0                  0             0             0            1   \n",
      "1                  0             0             0            1   \n",
      "2                  0             0             0            1   \n",
      "3                  0             0             0            1   \n",
      "4                  0             0             0            1   \n",
      "5                  0             0             0            1   \n",
      "6                  0             0             0            1   \n",
      "7                  0             0             0            1   \n",
      "8                  0             0             0            1   \n",
      "9                  0             0             0            1   \n",
      "10                 0             0             0            1   \n",
      "11                 0             1             0            0   \n",
      "12                 0             0             0            1   \n",
      "13                 0             1             0            0   \n",
      "14                 0             0             0            1   \n",
      "15                 0             0             0            1   \n",
      "16                 0             0             0            1   \n",
      "17                 0             0             0            1   \n",
      "18                 0             0             0            1   \n",
      "19                 0             0             0            0   \n",
      "20                 0             1             0            0   \n",
      "21                 0             0             0            1   \n",
      "22                 0             0             0            1   \n",
      "23                 0             0             0            1   \n",
      "24                 0             0             0            1   \n",
      "25                 0             0             0            1   \n",
      "26                 0             0             0            1   \n",
      "27                 0             0             0            1   \n",
      "28                 0             0             0            1   \n",
      "29                 0             0             0            1   \n",
      "...              ...           ...           ...          ...   \n",
      "1430               0             0             0            1   \n",
      "1431               0             0             0            1   \n",
      "1432               0             0             0            1   \n",
      "1433               0             0             0            1   \n",
      "1434               0             0             0            1   \n",
      "1435               0             0             0            0   \n",
      "1436               0             0             0            1   \n",
      "1437               0             1             0            0   \n",
      "1438               0             0             0            1   \n",
      "1439               0             0             0            1   \n",
      "1440               0             0             0            1   \n",
      "1441               0             0             0            1   \n",
      "1442               0             0             0            1   \n",
      "1443               0             0             0            1   \n",
      "1444               0             0             0            1   \n",
      "1445               0             0             0            1   \n",
      "1446               0             0             0            1   \n",
      "1447               0             0             0            1   \n",
      "1448               0             0             0            1   \n",
      "1449               0             0             0            1   \n",
      "1450               0             0             0            1   \n",
      "1451               0             1             0            0   \n",
      "1452               0             0             0            1   \n",
      "1453               0             0             0            1   \n",
      "1454               0             0             0            1   \n",
      "1455               0             0             0            1   \n",
      "1456               0             0             0            1   \n",
      "1457               0             0             0            1   \n",
      "1458               0             0             0            1   \n",
      "1459               0             0             0            1   \n",
      "\n",
      "      SaleCondition_Abnorml  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
      "0                         0                      0                     0   \n",
      "1                         0                      0                     0   \n",
      "2                         0                      0                     0   \n",
      "3                         1                      0                     0   \n",
      "4                         0                      0                     0   \n",
      "5                         0                      0                     0   \n",
      "6                         0                      0                     0   \n",
      "7                         0                      0                     0   \n",
      "8                         1                      0                     0   \n",
      "9                         0                      0                     0   \n",
      "10                        0                      0                     0   \n",
      "11                        0                      0                     0   \n",
      "12                        0                      0                     0   \n",
      "13                        0                      0                     0   \n",
      "14                        0                      0                     0   \n",
      "15                        0                      0                     0   \n",
      "16                        0                      0                     0   \n",
      "17                        0                      0                     0   \n",
      "18                        0                      0                     0   \n",
      "19                        1                      0                     0   \n",
      "20                        0                      0                     0   \n",
      "21                        0                      0                     0   \n",
      "22                        0                      0                     0   \n",
      "23                        0                      0                     0   \n",
      "24                        0                      0                     0   \n",
      "25                        0                      0                     0   \n",
      "26                        0                      0                     0   \n",
      "27                        0                      0                     0   \n",
      "28                        0                      0                     0   \n",
      "29                        0                      0                     0   \n",
      "...                     ...                    ...                   ...   \n",
      "1430                      0                      0                     0   \n",
      "1431                      0                      0                     0   \n",
      "1432                      0                      0                     0   \n",
      "1433                      0                      0                     0   \n",
      "1434                      0                      0                     0   \n",
      "1435                      1                      0                     0   \n",
      "1436                      0                      0                     0   \n",
      "1437                      0                      0                     0   \n",
      "1438                      0                      0                     0   \n",
      "1439                      0                      0                     0   \n",
      "1440                      0                      0                     0   \n",
      "1441                      0                      0                     0   \n",
      "1442                      0                      0                     0   \n",
      "1443                      0                      0                     0   \n",
      "1444                      0                      0                     0   \n",
      "1445                      0                      0                     0   \n",
      "1446                      0                      0                     0   \n",
      "1447                      0                      0                     0   \n",
      "1448                      0                      0                     0   \n",
      "1449                      1                      0                     0   \n",
      "1450                      0                      0                     0   \n",
      "1451                      0                      0                     0   \n",
      "1452                      0                      0                     0   \n",
      "1453                      1                      0                     0   \n",
      "1454                      0                      0                     0   \n",
      "1455                      0                      0                     0   \n",
      "1456                      0                      0                     0   \n",
      "1457                      0                      0                     0   \n",
      "1458                      0                      0                     0   \n",
      "1459                      0                      0                     0   \n",
      "\n",
      "      SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \n",
      "0                        0                     1                      0  \n",
      "1                        0                     1                      0  \n",
      "2                        0                     1                      0  \n",
      "3                        0                     0                      0  \n",
      "4                        0                     1                      0  \n",
      "5                        0                     1                      0  \n",
      "6                        0                     1                      0  \n",
      "7                        0                     1                      0  \n",
      "8                        0                     0                      0  \n",
      "9                        0                     1                      0  \n",
      "10                       0                     1                      0  \n",
      "11                       0                     0                      1  \n",
      "12                       0                     1                      0  \n",
      "13                       0                     0                      1  \n",
      "14                       0                     1                      0  \n",
      "15                       0                     1                      0  \n",
      "16                       0                     1                      0  \n",
      "17                       0                     1                      0  \n",
      "18                       0                     1                      0  \n",
      "19                       0                     0                      0  \n",
      "20                       0                     0                      1  \n",
      "21                       0                     1                      0  \n",
      "22                       0                     1                      0  \n",
      "23                       0                     1                      0  \n",
      "24                       0                     1                      0  \n",
      "25                       0                     1                      0  \n",
      "26                       0                     1                      0  \n",
      "27                       0                     1                      0  \n",
      "28                       0                     1                      0  \n",
      "29                       0                     1                      0  \n",
      "...                    ...                   ...                    ...  \n",
      "1430                     0                     1                      0  \n",
      "1431                     0                     1                      0  \n",
      "1432                     0                     1                      0  \n",
      "1433                     0                     1                      0  \n",
      "1434                     0                     1                      0  \n",
      "1435                     0                     0                      0  \n",
      "1436                     0                     1                      0  \n",
      "1437                     0                     0                      1  \n",
      "1438                     0                     1                      0  \n",
      "1439                     0                     1                      0  \n",
      "1440                     0                     1                      0  \n",
      "1441                     0                     1                      0  \n",
      "1442                     0                     1                      0  \n",
      "1443                     0                     1                      0  \n",
      "1444                     0                     1                      0  \n",
      "1445                     0                     1                      0  \n",
      "1446                     0                     1                      0  \n",
      "1447                     0                     1                      0  \n",
      "1448                     0                     1                      0  \n",
      "1449                     0                     0                      0  \n",
      "1450                     0                     1                      0  \n",
      "1451                     0                     0                      1  \n",
      "1452                     0                     1                      0  \n",
      "1453                     0                     0                      0  \n",
      "1454                     0                     1                      0  \n",
      "1455                     0                     1                      0  \n",
      "1456                     0                     1                      0  \n",
      "1457                     0                     1                      0  \n",
      "1458                     0                     1                      0  \n",
      "1459                     0                     1                      0  \n",
      "\n",
      "[1460 rows x 159 columns]\n"
     ]
    }
   ],
   "source": [
    "# It's most common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models. \n",
    "# Pandas offers a convenient function called get_dummies to get one-hot encodings.\n",
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "print(len(train_predictors),len(one_hot_encoded_training_predictors))\n",
    "print(one_hot_encoded_training_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error when Dropping Categoricals: 18651\n",
      "Mean Abslute Error with One-Hot Encoding: 18061\n"
     ]
    }
   ],
   "source": [
    "# Are the categorical model features (the one-hot encoded variables) helping the model?\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def get_mae(X, y):\n",
    "    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50), \n",
    "                                X, y, \n",
    "                                scoring = 'neg_mean_absolute_error').mean()\n",
    "\n",
    "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n",
    "\n",
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n",
    "\n",
    "print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\n",
    "print('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the test data is encoded in the same manner as the training data with the align command:\n",
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n",
    "                                                                    join='left', # Equivalent of SQL's left join: if there are ever columns that show up in one dataset and not the other, we will keep exactly the columns from our training data. \n",
    "                                                                    #join='inner', # Equivalent to SQL inner join: keeping only the columns showing up in both datasets. \n",
    "                                                                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. XGBoost: Gradient Boosted Decision Trees algorithm\n",
    "\n",
    "from https://www.kaggle.com/dansbecker/xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/Python3/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "data = pd.read_csv('./train.csv')\n",
    "data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "\n",
    "my_imputer = Imputer()\n",
    "train_X = my_imputer.fit_transform(train_X)\n",
    "test_X = my_imputer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To install, type on the terminal...\n",
    "# $ conda install -c conda-forge xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(train_X, train_y, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 16735.32272046233\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "predictions = my_model.predict(test_X)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hyperparameters Tuning\n",
    "\n",
    "XGBoost has a few hyperparameters that can dramatically affect your model's accuracy and training speed. The first hyperparameters you should understand are:\n",
    "\n",
    "##### n_estimators and early_stopping_rounds\n",
    "\n",
    "n_estimators specifies how many times to go through the modeling cycle described above.\n",
    "\n",
    "In the underfitting vs overfitting graph, n_estimators moves you further to the right. Too low a value causes underfitting, which is inaccurate predictions on both training data and new data. Too large a value causes overfitting, which is accurate predictions on training data, but inaccurate predictions on new data (which is what we care about). You can experiment with your dataset to find the ideal. Typical values range from 100-1000, though this depends a lot on the learning rate discussed below.\n",
    "\n",
    "The argument early_stopping_rounds offers a way to automatically find the ideal value. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
    "\n",
    "Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. early_stopping_rounds = 5 is a reasonable value. Thus we stop after 5 straight rounds of deteriorating validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n",
    "             eval_set=[(test_X, test_y)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using early_stopping_rounds, you need to set aside some of your data for checking the number of rounds to use. If you later want to fit a model with all of your data, set n_estimators to whatever value you found to be optimal when run with early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### learning_rate\n",
    "\n",
    "Here's a subtle but important trick for better XGBoost models:\n",
    "\n",
    "Instead of getting predictions by simply adding up the predictions from each component model, we will multiply the predictions from each model by a small number before adding them in. This means each tree we add to the ensemble helps us less. In practice, this reduces the model's propensity to overfit.\n",
    "\n",
    "So, you can use a higher value of n_estimators without overfitting. If you use early stopping, the appropriate number of trees will be set automatically.\n",
    "\n",
    "In general, a small learning rate (and large number of estimators) will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "my_model.fit(train_X, train_y, early_stopping_rounds=5, \n",
    "             eval_set=[(test_X, test_y)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n_jobs\n",
    "\n",
    "On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help.\n",
    "\n",
    "The resulting model won't be any better, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Partial-Dependence Plots\n",
    "\n",
    "These type of plots show how each variable or predictor affects the model's predictions.\n",
    "\n",
    "from https://www.kaggle.com/dansbecker/partial-dependence-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "cols_to_use = ['Distance', 'Landsize', 'BuildingArea']\n",
    "\n",
    "def get_some_data():\n",
    "    data = pd.read_csv('./melb_data.csv')\n",
    "    y = data.Price\n",
    "    X = data[cols_to_use]\n",
    "    my_imputer = Imputer()\n",
    "    imputed_X = my_imputer.fit_transform(X)\n",
    "    return imputed_X, y\n",
    "    \n",
    "\n",
    "X, y = get_some_data()\n",
    "my_model = GradientBoostingRegressor()\n",
    "my_model.fit(X, y)\n",
    "my_plots = plot_partial_dependence(my_model, \n",
    "                                   features=[0,2], \n",
    "                                   X=X, \n",
    "                                   feature_names=cols_to_use, \n",
    "                                   grid_resolution=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "\n",
    "# get_some_data is defined in cell above.\n",
    "X, y = get_some_data()\n",
    "# scikit-learn originally implemented partial dependence plots only for Gradient Boosting models\n",
    "# this was due to an implementation detail, and a future release will support all model types.\n",
    "my_model = GradientBoostingRegressor()\n",
    "# fit the model as usual\n",
    "my_model.fit(X, y)\n",
    "# Here we make the plot\n",
    "my_plots = plot_partial_dependence(my_model,       \n",
    "                                   features=[0, 2], # column numbers of plots we want to show\n",
    "                                   X=X,            # raw predictors data.\n",
    "                                   feature_names=['Distance', 'Landsize', 'BuildingArea'], # labels on graphs\n",
    "                                   grid_resolution=10) # number of values to plot on x axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tips related to plot_partial_dependence:\n",
    "\n",
    "- The features are the column numbers from the X array or dataframe that you wish to have plotted. This starts to look bad beyond 2 or 3 variables. You could make repeated calls to plot 2 or 3 at a time.\n",
    "\n",
    "- There are options to establish what points on the horizontal axis are plotted. The simplest is grid_resolution which we use to determine how many different points are plotted. These plots tend to look jagged as that value increases, because you will pick up lots of randomness or noise in your model. It's best not to take the small or jagged fluctuations too literally. Smaller values of grid_resolution smooth this out. It's also much less of an issue for datasets with many rows.\n",
    "\n",
    "- There is a function called partial_dependence to get the raw data making up this plot, rather than making the visual plot itself. This is useful if you want to control how it is visualized using a plotting package like Seaborn. With moderate effort, you could make much nicer looking plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pipelines\n",
    "\n",
    "A pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step. Benefits include cleaner and more elegant code, fewer bugs, easier-to-productionize code, and more options for model testing (e.g., cross validation).\n",
    "\n",
    "from https://www.kaggle.com/dansbecker/pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read Data\n",
    "data = pd.read_csv('./melb_data.csv')\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "y = data.Price\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundling a modeling process that uses...\n",
    "# - an Imputer to fill in missing values, with \n",
    "# - a RandomForestRegressor to make predictions. \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())  # Pipeline must start with transformers-object steps and end with a models-object step.\n",
    "\n",
    "# Use the Pipeline as a fused whole\n",
    "my_pipeline.fit(train_X, train_y)\n",
    "predictions = my_pipeline.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, here is the code to do the same thing without pipelines:\n",
    "    \n",
    "    my_imputer = Imputer()\n",
    "    my_model = RandomForestRegressor()\n",
    "\n",
    "    imputed_train_X = my_imputer.fit_transform(train_X)\n",
    "    imputed_test_X = my_imputer.transform(test_X)\n",
    "    my_model.fit(imputed_train_X, train_y)\n",
    "    predictions = my_model.predict(imputed_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cross-Validation\n",
    "\n",
    "if our dataset is smaller, we should run cross-validation. Otherwise, a simple train-test split is sufficient for larger datasets. \n",
    "\n",
    "There's no simple threshold for what constitutes a large vs small dataset. If your model takes a couple minute or less to run, it's probably worth switching to cross-validation.\n",
    "\n",
    "Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment gives the same results, train-test split is probably sufficient.\n",
    "\n",
    "from https://www.kaggle.com/dansbecker/cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./melb_data.csv')\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = data[cols_to_use]\n",
    "y = data.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a pipeline of modeling steps\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "my_pipeline = make_pipeline(Imputer(), RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-312865.49752141 -308338.11133546 -296051.08715304 -248864.4014016\n",
      " -266151.19477723]\n"
     ]
    }
   ],
   "source": [
    "# Get cross-validation scores\n",
    "# Recall that Scikit-learn has a convention where all metrics are defined so a high number is better (that's why NEGATIVE mean_abs_error.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(my_pipeline, X, y, cv=5,scoring='neg_mean_absolute_error')  # For other scoring arguments, see http://scikit-learn.org/stable/modules/model_evaluation.html \n",
    "print(scores)  # Note that if cv is not given, cross_val_score will use cv=3 folds by default, that is one holdout fold for validation and two holdin folds for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 286454.058438\n"
     ]
    }
   ],
   "source": [
    "# Obtain single measure of model quality to compare between models. So we take the average across experiments.\n",
    "print('Mean Absolute Error %2f' %(-1 * scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Leakage\n",
    "\n",
    "There are two main types of leakage: Leaky Predictors and a Leaky Validation Strategies.\n",
    "\n",
    "**Leaky Predictors** occur when your predictors include data that will not be available at the time you make predictions. To prevent it, any variable updated (or created) after the target value is realized should be excluded. Because when we use this model to make new predictions, that data won't be available to the model.\n",
    "\n",
    "For **Leaky Validation Strategies**, consider that validation is meant to be a measure of how the model does on data it hasn't considered before. One can corrupt this process in subtle ways if the validation data affects the preprocessing behavoir. For example, this happens if you run preprocessing (like fitting the Imputer for missing values) before calling train_test_split. To prevent Leaky Validation Strategies, do the preprocessing inside a pipeline. If your validation is based on a simple train-test split, exclude the validation data from any type of fitting, including the fitting of preprocessing steps. \n",
    "\n",
    "\n",
    "from https://www.kaggle.com/dansbecker/data-leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "We will use a small dataset about credit card applications, and we will build a model predicting which applications were accepted (stored in a variable called card)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   card  reports       age  income     share  expenditure  owner  selfemp  \\\n",
      "0  True        0  37.66667  4.5200  0.033270   124.983300   True    False   \n",
      "1  True        0  33.25000  2.4200  0.005217     9.854167  False    False   \n",
      "2  True        0  33.66667  4.5000  0.004156    15.000000   True    False   \n",
      "3  True        0  30.50000  2.5400  0.065214   137.869200  False    False   \n",
      "4  True        0  32.16667  9.7867  0.067051   546.503300   True    False   \n",
      "\n",
      "   dependents  months  majorcards  active  \n",
      "0           3      54           1      12  \n",
      "1           3      34           1      13  \n",
      "2           4      58           1       5  \n",
      "3           0      25           1       7  \n",
      "4           2      64           1       5  \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./AER_credit_card_data.csv', \n",
    "                   true_values = ['yes'],\n",
    "                   false_values = ['no'])\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1319, 12)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy: 0.978011\n"
     ]
    }
   ],
   "source": [
    "# We can see with data.shape that this is a small dataset (1319 rows), \n",
    "# so we should use cross-validation to ensure accurate measures of model quality\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y = data.card\n",
    "X = data.drop(['card'], axis=1)\n",
    "\n",
    "# Since there was no preprocessing, we didn't need a pipeline here. Used anyway as best practice\n",
    "modeling_pipeline = make_pipeline(RandomForestClassifier())\n",
    "cv_scores = cross_val_score(modeling_pipeline, X, y, scoring='accuracy')\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With experience, you'll find that it's very rare to find models that are accurate 98% of the time. It happens, but it's rare enough that we should inspect the data more closely to see if it is target leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of the data, which you can also find under the data tab:\n",
    "\n",
    "    card: Dummy variable, 1 if application for credit card accepted, 0 if not\n",
    "    reports: Number of major derogatory reports\n",
    "    age: Age n years plus twelfths of a year\n",
    "    income: Yearly income (divided by 10,000)\n",
    "    share: Ratio of monthly credit card expenditure to yearly income\n",
    "    expenditure: Average monthly credit card expenditure\n",
    "    owner: 1 if owns their home, 0 if rent\n",
    "    selfempl: 1 if self employed, 0 if not.\n",
    "    dependents: 1 + number of dependents\n",
    "    months: Months living at current address\n",
    "    majorcards: Number of major credit cards held\n",
    "    active: Number of active credit accounts\n",
    "\n",
    "A few model features look suspicious. For example, does expenditure mean expenditure on this card or on cards used before appying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of those who received a card with no expenditures: 0.02\n",
      "Fraction of those who did NOT received a card with no expenditures: 1.00\n"
     ]
    }
   ],
   "source": [
    "expenditures_cardholders = data.expenditure[data.card]\n",
    "expenditures_noncardholders = data.expenditure[~data.card]\n",
    "\n",
    "print('Fraction of those who received a card with no expenditures: %.2f' \\\n",
    "      %(( expenditures_cardholders == 0).mean()))  # Sum of all True values (1's) divided by the number of rows\n",
    "print('Fraction of those who did NOT received a card with no expenditures: %.2f' \\\n",
    "      %((expenditures_noncardholders == 0).mean())) # Sum of all True values (1's) divided by the number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems a data leak, where expenditures probably means **expenditures on the card they applied for.**.\n",
    "\n",
    "Since **share** is partially determined by **expenditure**, it should be excluded too. \n",
    "\n",
    "The variables **active** and **majorcards** are a little less clear, but from the description, they sound concerning. \n",
    "\n",
    "In most situations, it's better to be safe than sorry if you can't track down the people who created the data to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-val accuracy: 0.808187\n"
     ]
    }
   ],
   "source": [
    "# Run a model without leakage\n",
    "potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n",
    "X2 = X.drop(potential_leaks, axis=1)\n",
    "cv_scores = cross_val_score(modeling_pipeline, X2, y, scoring='accuracy')\n",
    "print(\"Cross-val accuracy: %f\" %cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is quite a bit lower, which on the one hand is disappointing. However, we can expect it to be right about 80% of the time when used on new applications, whereas the leaky model would likely do much worse then that (even in spite of it's higher apparent score in cross-validation.).\n",
    "\n",
    "**In Conclusion:** Data leakage can be multi-million dollar mistake in many data science applications. Careful separation of training and validation data is a first step, and pipelines can help implement this separation. Leaking predictors are a more frequent issue, and leaking predictors are harder to track down. A combination of caution, common sense and data exploration can help identify leaking predictors so you remove them from your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Advanced Pipelines & Grid Search Cross Validation\n",
    "\n",
    "from https://www.kaggle.com/aashita/advanced-pipelines-tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Read Data\n",
    "data = pd.read_csv('./melb_data.csv')\n",
    "cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n",
    "X = np.array(data[cols_to_use])\n",
    "y = data.Price\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Pipeline\n",
    "# Use Imputer to fill in missing values, followed by a XGBRegressor to make predictions.\n",
    "# Use Pipeline (instead of make_pipeline) to have a name for every step in our pipeline, so that we can call on a step and set parameters.\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "my_pipeline = Pipeline([ ('imputer', Imputer()), ('xgbrg', XGBRegressor()) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('xgbrg', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_dep...       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1))]),\n",
       "       fit_params={'xgbrg__eval_set': [(array([[3.00e+00, 1.20e+01, ...,      nan,      nan],\n",
       "       [4.00e+00, 9.30e+00, ..., 2.15e+02, 1.97e+03],\n",
       "       ...,\n",
       "       [2.00e+00, 1.60e+00, ...,      nan,      nan],\n",
       "       [5.00e+00, 9.90e+00, ..., 1.66e+02, 1.95e+03]]), 12825     900000.0\n",
       "1857     1570000.0... Price, Length: 3395, dtype: float64)], 'xgbrg__early_stopping_rounds': 10, 'xgbrg__verbose': False},\n",
       "       iid=True, n_jobs=1,\n",
       "       param_grid={'xgbrg__n_estimators': [10, 50, 100, 500], 'xgbrg__learning_rate': [0.1, 0.5, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV for tuning the model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# dictionary for the values of parameters that we want to compare\n",
    "param_grid = {\n",
    "    \"xgbrg__n_estimators\": [10, 50, 100, 500],\n",
    "    \"xgbrg__learning_rate\": [0.1, 0.5, 1],\n",
    "}\n",
    "\n",
    "# add the name of the regression (here xgbrg__) in front of the parameters' names in the param_grid \n",
    "fit_params = {\"xgbrg__eval_set\": [(val_X, val_y)], \n",
    "              \"xgbrg__early_stopping_rounds\": 10, \n",
    "              \"xgbrg__verbose\": False} \n",
    "\n",
    "# using 5-fold cross validation, cv = 5\n",
    "searchCV = GridSearchCV(my_pipeline, cv=5, param_grid=param_grid, fit_params=fit_params)\n",
    "searchCV.fit(train_X, train_y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the number of trees in XGBoost models, that is n_estimators, are tuned by using early_stopping_rounds. The early stopping is decided by checking the prediction of the trained models on a validation set, and hence it is required that we pass an eval_set alongside the early_stopping_rounds in the fit_params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One can also sue Random Search for tuning the model\n",
    "    \n",
    "    from sklearn.preprocessing import Imputer\n",
    "    #from xgboost import XGBClassifier\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    import scipy.stats as st \n",
    "\n",
    "    #my_pipeline2 = Pipeline([('imputer',  Imputer()),  ('xgbcl',  XGBClassifier())])\n",
    "    my_pipeline = Pipeline([ ('imputer', Imputer()), ('xgbrg', XGBRegressor()) ])\n",
    "\n",
    "    params2 = { \"xgbcl__n_estimators\":  st.randint(3, 50),\n",
    "               \"xgbcl__learning_rate\":  st.uniform(0.05, 1)}\n",
    "    fit_params2 =  {\"xgbcl__eval_set\": [(val_X, val_y)],            \n",
    "                   \"xgbcl__early_stopping_rounds\":  10} \n",
    "\n",
    "    searchCV2 =  RandomizedSearchCV(my_pipeline2,  cv=5,  param_distributions=params2, \n",
    "                                   fit_params=fit_params2)\n",
    "    searchCV2.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgbrg__learning_rate': 0.1, 'xgbrg__n_estimators': 50}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the best hyperparameters\n",
    "searchCV.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04802058, 0.58715505, 0.58715505, 0.58715505, 0.57831235,\n",
       "       0.57831235, 0.57831235, 0.57831235, 0.55767936, 0.55923833,\n",
       "       0.55923833, 0.55923833])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchCV.cv_results_['mean_train_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03770821, 0.55842203, 0.55842203, 0.55842203, 0.5499774 ,\n",
       "       0.5499774 , 0.5499774 , 0.5499774 , 0.53095758, 0.53219691,\n",
       "       0.53219691, 0.53219691])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchCV.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5298441230570536, 0.503369349220988)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchCV.cv_results_['mean_train_score'].mean(), searchCV.cv_results_['mean_test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
